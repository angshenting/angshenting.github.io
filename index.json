[{"content":"The SCBA Ratings page is live!\nThus far, only matchpoint pairs games are rated, from 2023 onwards. Most of the development was done in a week with the help of ChatGPT and Claude.\nWhy 2023? Given that SCBA re-opened in May 2022 after COVID-19 restrictions were eased, 2023 seems like a good starting point when most people returned to weekly games.\nHow is this done? Ratings are implemented using OpenSkill, using Plackett-Luce based on rankings.\nWhat about teams? In the process of implementation, most likely using Glicko-2 (see previous post) on head to head matches with weighting for number of boards.\n","permalink":"https://shenting.org/post/scba_rating_live/","summary":"\u003cp\u003e\u003ca href=\"https://dev1.scba.org.sg/ratings\"\u003eThe SCBA Ratings page is live!\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThus far, only matchpoint pairs games are rated, from 2023 onwards. Most of the development was done in a week with the help of ChatGPT and Claude.\u003c/p\u003e\n\u003ch2 id=\"why-2023\"\u003eWhy 2023?\u003c/h2\u003e\n\u003cp\u003eGiven that SCBA re-opened in May 2022 after COVID-19 restrictions were eased, 2023 seems like a good starting point when most people returned to weekly games.\u003c/p\u003e\n\u003ch2 id=\"how-is-this-done\"\u003eHow is this done?\u003c/h2\u003e\n\u003cp\u003eRatings are implemented using \u003ca href=\"https://openskill.me/en/stable/\"\u003eOpenSkill\u003c/a\u003e, using Plackett-Luce based on rankings.\u003c/p\u003e","title":"SCBA Ratings - Now Live"},{"content":"Teams of Varying Sizes TrueSkill was first developed by Microsoft Research for use in multiplayer team games, like Halo. It is a Bayesian ranking system which is suited for:\nTeams of varying sizes Multiplayer games Dynamic updating after each match Mathematics of TrueSkill Each player $i$ is modeled with a skill represented by a normal distribution:\n$$ \\theta_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2) $$\n$\\mu_i$ = current estimate of player skill (mean) $\\sigma_i = uncertainty about that skill (standard deviation) At the start, players are typically initialized with:\n$$ \\mu_0 = 25,\\quad \\sigma_0 = \\frac{25}{3} \\approx 8.33 $$\nMatch Outcomes Given the skills of players or teams, the performance in a match is drawn from:\n$$ p_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2 + \\beta^2) $$\nwhere $ \\beta $ controls the variance of performance — modeling \u0026ldquo;luck\u0026rdquo;.\nThe outcome of a match constrains the latent performance variables:\n$$ p_i \u0026gt; p_j \\quad \\text{(if player i beats player j)} $$\nTrueSkill performs Bayesian inference to update the posterior distributions of $ \\mu_i $ and $ \\sigma_i $ accordingly.\nUpdating Ratings Given observed match outcomes, TrueSkill uses the following update formulas (simplified):\nMean update: $$ \\mu_i\u0026rsquo; = \\mu_i + \\frac{\\sigma_i^2}{c} \\cdot v $$\nVariance update: $$ \\sigma_i\u0026rsquo;^2 = \\sigma_i^2 \\cdot \\left( 1 - \\frac{\\sigma_i^2}{c^2} \\cdot w \\right) $$\nWhere:\n$ c $ is a function of combined uncertainties. $ v $ and $ w $ are derived from the cumulative density function (CDF) and probability density function (PDF) of the standard normal distribution, depending on the margin of victory. These formulas propagate uncertainty reduction over time — strong repeated performance will lower $ \\sigma $, making the rating more confident.\nHandling of Teams Each team is modelled as the sum of its players\u0026rsquo; skills. Players within the same team update jointly after a match. Any number of players per team is supported. Comparison with Glicko and Elo System Uncertainty model Teams support Multiplayer Draw support Elo No No No Yes Glicko-2 Yes Limited No Yes TrueSkill Yes Yes Yes Yes OpenSkill OpenSkill is an open-source Python package which implements TrueSkill\n","permalink":"https://shenting.org/post/rating_3_trueskill/","summary":"\u003ch1 id=\"teams-of-varying-sizes\"\u003eTeams of Varying Sizes\u003c/h1\u003e\n\u003cp\u003eTrueSkill was first developed by Microsoft Research for use in multiplayer team games, like Halo. It is a Bayesian ranking system which is suited for:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTeams\u003c/strong\u003e of varying sizes\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMultiplayer\u003c/strong\u003e games\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic updating\u003c/strong\u003e after each match\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"mathematics-of-trueskill\"\u003eMathematics of TrueSkill\u003c/h1\u003e\n\u003cp\u003eEach player $i$ is modeled with a skill represented by a normal distribution:\u003c/p\u003e\n\u003cp\u003e$$\n\\theta_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\mu_i$ = current estimate of player skill (mean)\u003c/li\u003e\n\u003cli\u003e$\\sigma_i = uncertainty about that skill (standard deviation)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAt the start, players are typically initialized with:\u003c/p\u003e","title":"Rating Systems (3): TrueSkill - Multiplayer Ratings"},{"content":"The Death of the American Dream I recall watching the movie adaptation of The Great Gatsby back in 2013. Baz Luhrmann created a visual spectacle, featuring Leonardo DiCaprio as the titular character who has become part of multiple memes. And who can forget Lana Del Rey\u0026rsquo;s haunting vocals for the theme \u0026ldquo;Young and Beautiful\u0026rdquo;? I read the original novel soon after, and was pretty struck by how deep the underlying themes run. Fast forward to 2025, and it has been exactly a century since F. Scott Fitzgerald published The Great Gatsby. One hundred years later, its themes feel eerily familiar. The excessive wealth, the disillusionment, the relentless pursuit of a dream—all of it maps almost perfectly onto today’s tech-fueled, crypto-charged, influencer-driven landscape. This struck me on a commute home last week and as I thought about it, the main characters do reflect segments of modern day society.\nTom Buchanan = Trump/Vance/Musk etc. Tom is the gatekeeper of old power. He is loud, aggressive, and entitled—living proof that wealth doesn’t equal class. He dominates conversations and people, and he always lands on his feet. He is brutish and cruel, and infidelity comes naturally to him.\nDaisy Buchanan = Old Money/GOP Daisy is the ultimate prize—beautiful, elusive, and hollow. Gatsby\u0026rsquo;s fixation isn\u0026rsquo;t with her personality, but with the status and security she represents. In 2025, Daisy represents the old political elite, the traditional Republican establishment, the donor class, and old money. She doesn’t take action; she lets others fight for her attention and then quietly retreats to her insulated world. Myrtle Wilson = Poor Populist/MAGA supporters Myrtle wants out. She wants to escape the greyness of her world and step into one of glamor and power. But she’s used, discarded, and ultimately destroyed by the very system she tries to join. She chases after Tom, believing that he will bring her a life of glamour, but only to be run over by Daisy who is driving Gatsby\u0026rsquo;s car.\nGeorge Wilson = Other Poor Blissfully ignorant of the affair his wife (fellow poor) are having with Tom, a man from Old Money, he becomes a pawn in the rivalry between Tom and Gatsby and ends up blaming and killing Gatsby for the death of his wife, before ending his own. Who then does Gatsby represent?\nJay Gatsby = Crypto Bros/Tech Founders Gatsby was a self-made man, though through means which are not exactly deemed as completely honest. His pursuit of Daisy represents his desire for status in society; similarly, the glitzy parties he threw were to chase the image. Does he really love Daisy? Probably not as much as he is in love with the ideal of what Daisy represented. Today\u0026rsquo;s Gatsby is the tech founder or crypto bro chasing legitimacy and glory. He came from nothing, made his millions (or billions) through unconventional means, and now seeks social capital to match his financial one. He is obsessed with status symbols, but beneath the surface lies a gnawing emptiness—a desire to be seen and accepted by a world that may never truly embrace him.\nConclusion Fitzgerald\u0026rsquo;s tale is timeless - humans have been chasing glamour, status and power. However, history does seem to run in cycles - as the world today seems to be on a precipice into a Great Depression-style recession, wars raging/threatening to rage around the world - we seem to have come full circle back into the disenchantment and the end of an era. If we follow the storyline, it\u0026rsquo;s clear who we should not be - Gatsby\u0026rsquo;s tale of dream-chasing ends up in tragedy and he gets blamed for the death of Myrtle even though he is innocent, and almost nobody really mourns his death. If you agree with the above, then perhaps, like me, you\u0026rsquo;ve become Nick and maybe you might be haunted by the death of Gatsby in the future. P.S. Who Jordan Baker represents is left as an open question - I have my own ideas.\n","permalink":"https://shenting.org/post/thegreatgatsby/","summary":"\u003ch1 id=\"the-death-of-the-american-dream\"\u003eThe Death of the American Dream\u003c/h1\u003e\n\u003cp\u003eI recall watching the movie adaptation of The Great Gatsby back in 2013. Baz Luhrmann created a visual spectacle, featuring Leonardo DiCaprio as the titular character who has become part of multiple memes. And who can forget Lana Del Rey\u0026rsquo;s haunting vocals for the theme \u0026ldquo;Young and Beautiful\u0026rdquo;? I read the original novel soon after, and was pretty struck by how deep the underlying themes run.\nFast forward to 2025, and it has been exactly a century since F. Scott Fitzgerald published The Great Gatsby. One hundred years later, its themes feel eerily familiar. The excessive wealth, the disillusionment, the relentless pursuit of a dream—all of it maps almost perfectly onto today’s tech-fueled, crypto-charged, influencer-driven landscape. This struck me on a commute home last week and as I thought about it, the main characters do reflect segments of modern day society.\u003c/p\u003e","title":"The Great Gatsby: 100 Years Later"},{"content":"Yep, just one week into the new year of 2025 I fractured my right last finger during football, during the seemingly innocuous act of catching a ball.\nHaving been told for years the stereotype of \u0026ldquo;you\u0026rsquo;ll know if it is a fracture by the intense pain\u0026rdquo;, I thought it was just a bad sprain, because the pain was aching rather than sharp. There was no swelling, no bruising, so I just went for dinner as usual, and then back home, and that was when the swelling really started.\nSo it was a midnight trip to A\u0026amp;E, an x-ray showed there was indeed one, but I was told it was \u0026ldquo;hairline\u0026rdquo; (nope) and sent home with painkillers and a splint at 4am. Got myself an appointment in the morning with a hand specialist (recommended) and got a same day appointment, only to be told that afternoon that the finger is misaligned because the fracture was large enough to affect the ligament alignment and I need surgery.\nSurgery wasn\u0026rsquo;t too bad really, other than having to be at hospital at 0530. The surgery itself was quick, and after having some food and making sure that I was producing urine fine, I was discharged with painkillers, hospitalisation leave and post-op X-rays. So yes, I am now 0.01% titanium.\nThe two weeks of leave (with CNY inside) were just a fog of painkillers. Stitches out after two weeks, dressing fully out after another (to give the slightly raw wound more time to dry and scab) and now this week the scab and some dead skin is finally coming off. Oddly enough, it\u0026rsquo;s more painful now than those two weeks with stitches - therapy requires a lot of bending of the injured finger to regain the motion and this is not easy. That said, I am now allowed to type with the injured finger - not that the right last finger sees much action.\nNot quite the start of 2025 I was expecting.\n","permalink":"https://shenting.org/post/fracture/","summary":"\u003cp\u003eYep, just one week into the new year of 2025 I fractured my right last finger during football, during the seemingly innocuous act of catching a ball.\u003c/p\u003e\n\u003cp\u003eHaving been told for years the stereotype of \u0026ldquo;you\u0026rsquo;ll know if it is a fracture by the intense pain\u0026rdquo;, I thought it was just a bad sprain, because the pain was aching rather than sharp. There was no swelling, no bruising, so I just went for dinner as usual, and then back home, and that was when the swelling really started.\u003c/p\u003e","title":"Fracture"},{"content":"You probably also hate wasting time at meetings. What I don\u0026rsquo;t understand is how some meetings can go on for multiple instances without proper presentation skills - appallingly this is a meeting of senior technical leads.\nLook, it doesn\u0026rsquo;t even have to be BPMN, although that\u0026rsquo;s a good start. I don\u0026rsquo;t understand how half of the meeting revolved around a draft of some standards that was never screen-shared and the meeting spent 30 minutes talking (in circles). The second half of the meeting showed a dashboard (from vendor, not customized), and the presenter was trying to show some trends, but there was no narrative, no custom dashboard or some sort of analysis to demonstrate the hypotheses, and then some proposed scheme of action buried in an email (like, can\u0026rsquo;t you just copy it to a Powerpoint slide and even make it point form?!) that was convoluted enough that it was unclear what was really happening.\nOf course, it could just well be that the presenters don\u0026rsquo;t really know what they are doing\u0026hellip;\n","permalink":"https://shenting.org/post/visualizations/","summary":"\u003cp\u003eYou probably also hate wasting time at meetings. What I don\u0026rsquo;t understand is how some meetings can go on for multiple instances without proper presentation skills - appallingly this is a meeting of senior technical leads.\u003c/p\u003e\n\u003cp\u003eLook, it doesn\u0026rsquo;t even have to be \u003ca href=\"https://www.bpmn.org/\"\u003eBPMN\u003c/a\u003e, although that\u0026rsquo;s a good start. I don\u0026rsquo;t understand how half of the meeting revolved around a draft of some standards that was never screen-shared and the meeting spent 30 minutes talking (in circles). The second half of the meeting showed a dashboard (from vendor, not customized), and the presenter was trying to show some trends, but there was no narrative, no custom dashboard or some sort of analysis to demonstrate the hypotheses, and then some proposed scheme of action buried in an email (like, can\u0026rsquo;t you just copy it to a Powerpoint slide and even make it point form?!) that was convoluted enough that it was unclear what was really happening.\u003c/p\u003e","title":"Visualizations at Meetings"},{"content":"Imagine you’re a newcomer to Python and data analytics and some website tells you to use conda. Days later, you get an email from Anaconda telling you that you’re in breach of their licensing terms because the organisation you’re working for has more than 200 employees!\nConfused, you do a quick google and find this: https://www.anaconda.com/blog/is-conda-free\nNow you’re even more confused.\nMeanwhile on the second search result, the answer is clearer: https://stackoverflow.com/questions/74762863/are-conda-miniconda-and-anaconda-free-to-use-and-open-source\nIf something named “default” is not really free unconditionally, perhaps the software isn’t free.\nIf a company releases something for free, then changes the terms suddenly to make it no longer free, then it just creates legal risks for people to continue using it.\nIf the licensing terms make it such that 1 employee in a 1000 strong organisation is on the hook for using conda, but all 199 employees in a 199 strong organisation is not in breach, then whoever decided on these regulations is not being smart about this.\n(Yeah I get it, the company needs to make money, but there are better ways of licensing that better correlates with usage than a blunt instrument of company size as a cut-off.)\nMost importantly: Conda doesn’t necessarily solve all of the issues associated with using pip, and it is better in the long-term for someone new to Python to learn how to use pip anyway. There might be situations where conda can’t be/should not be used.\nConclusion: Just use pip. Do not use conda.\n","permalink":"https://shenting.org/post/anaconda/","summary":"\u003cp\u003eImagine you’re a newcomer to Python and data analytics and some website tells you to use conda. Days later, you get an email from Anaconda telling you that you’re in breach of their licensing terms because the organisation you’re working for has more than 200 employees!\u003c/p\u003e\n\u003cp\u003eConfused, you do a quick google and find this: \u003ca href=\"https://www.anaconda.com/blog/is-conda-free\"\u003ehttps://www.anaconda.com/blog/is-conda-free\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eNow you’re even more confused.\u003c/p\u003e\n\u003cp\u003eMeanwhile on the second search result, the answer is clearer: \u003ca href=\"https://stackoverflow.com/questions/74762863/are-conda-miniconda-and-anaconda-free-to-use-and-open-source\"\u003ehttps://stackoverflow.com/questions/74762863/are-conda-miniconda-and-anaconda-free-to-use-and-open-source\u003c/a\u003e\u003c/p\u003e","title":"You Should not be Using Anaconda"},{"content":"I gave a talk last night at Data Science SG entitled “Trustable Data: Challenges in a National Sports Association”. It gives an outline of what I’ve encountered and done in the past few years for SCBA. Talk slides can be found here\n","permalink":"https://shenting.org/post/datachallenges/","summary":"\u003cp\u003eI gave a talk last night at Data Science SG entitled “Trustable Data: Challenges in a National Sports Association”. It gives an outline of what I’ve encountered and done in the past few years for SCBA. \u003ca href=\"https://docs.google.com/presentation/d/1h88SZ2S25xFc9JAjCdBfWVK-edYJj_btPIN5gI2Za-A/edit\"\u003eTalk slides can be found here\u003c/a\u003e\u003c/p\u003e","title":"DataScience SG Talk: Data Challenges"},{"content":"Koo writes: \u0026ldquo;In lifelong learning, we are expecting the participants to be able to apply what is being taught into their work. Applications to generate value is the key objectives for lifelong learning programmes. Assessment can conducted if it is on the application phase but unnecessary (but good to have) if it is to check if the participants have gained the knowledge needed from the course. In fact, mentoring might be more important as it guides participants, with an unorganized knowledge base as mentioned above, to start organizing the knowledge base and see where the applications of the knowledge are at the same time. However, this is difficult again due to cost issue. Yes, current experienced staff can be the mentor but they are already swarmed with their own work. Hiring external mentor could be a solution but again, opportunity cost for the freelancer can be high if the company only require an hour from the external mentor for guidance.\u0026rdquo;\nGiven that my adult training experience mostly consists of only teaching new tournament directors for bridge, I\u0026rsquo;m probably slightly better qualified to talk about mentorship. And as I write this, I\u0026rsquo;ve just taken on my third ever intern (and second here in this role), so I don\u0026rsquo;t claim deep expertise either!\nMentorship is hard, and \u0026ldquo;unorganized\u0026rdquo; is pretty much a good way of describing it. The following can all be mentorship to varying levels:\nCasual chat at a networking event\nMore in-depth 1-1 (possibly followup from above)\n\u0026ldquo;Formal\u0026rdquo; mentorship programmes (e.g. Data Science SG previously)\nLong-term friendship/mentorship\nActual workplace mentorship, e.g. internship\nThe other reason why it\u0026rsquo;s \u0026ldquo;unorganized\u0026rdquo; - mentorship is very individualized and every mentor/mentee dynamic is different:\nContext of the mentorship\nAims of the mentee (and the mentor!)\nArea of mentorship\nHow I\u0026rsquo;ve tried to mentor What does the mentee want? The first question will usually illuminate what the aims of the mentee are. The odds are good that they are not asking for general advice.\nDo I have the right experience/expertise? Not all the time, and I\u0026rsquo;m not afraid to say so or to point the mentee to other sources.\nI don\u0026rsquo;t see a mentor as being the GPS system for the mentee, telling them where to go, but more of an (imperfect) map, showing possible routes that the mentor themselves already know of that the mentee might not be aware of. But first, the mentee needs to tell the mentor where they wish to get to.\nWorkplace/Long-term mentorship The first intern I had was back in 2015, and I definitely felt the massive imposter syndrome then, being relatively junior as a data scientist myself. I recall my throught processes back then being:\nGive a task which I have done before which is relatively easy, but involves the whole pipeline (data preprocessing, fitting a model, and presenting results)\nBrief the task, take in any questions\nAllow her to try the task without any intervention unless needed\nReview, pointing out any improvements if needed\nThis worked pretty well, so iterating on this, my general idea would be to give a fairly good sized task using a dataset, and then let the intern work on it while providing any support when and where needed. I also like to ask questions to guide discovery, as I feel it guides the thinking processes better than just simply feeding answers. Even if the question fails, it allows me to explain the thought process I would have when approaching the problem.\nThat said though, actual work is half of the mentorship. Usually an intern would have decisions to make regarding next steps, e.g.: pursue a Masters degree/PhD? Stay in Singapore or go overseas? etc. Earlier this year I had to write reference letters for my previous intern, and we spent quite a bit of time previously discusisng her further plans, down to the details of which schools/programmes in the US she should be targetting. Good thing for both of us that I had some expertise/experience to advise her!\nThat said, it’s always good to seek mentors outside of the workplace who can bring a different perspective. It’s also useful for other soft skills that might otherwise not be so prevalent in the specific industry.\nAcknowledgements It was a tough struggle for me a decade ago when I was considering doing a PhD, and I was thankful to have both my direct supervisor, Yvonne, and also Shaowei both giving me good advice both on the actual work and on my next steps. Yvonne was very encouraging on non-PhD pathways when I made the decision to give up on a PhD, and Shaowei repeatedly gave me encouragement and feedback on my insights and ideas. I hope that I\u0026rsquo;m channeling their great mentorship as best as I can as a mentor now myself.\n","permalink":"https://shenting.org/post/mentorship/","summary":"\u003cp\u003e\u003ca href=\"https://koopingshung.com/blog/education-vs-lifelong-learning/\"\u003eKoo writes:\u003c/a\u003e\n\u0026ldquo;In lifelong learning, we are expecting the participants to be able to apply what is being taught into their work. Applications to generate value is the key objectives for lifelong learning programmes. Assessment can conducted if it is on the application phase but unnecessary (but good to have) if it is to check if the participants have gained the knowledge needed from the course. In fact, mentoring might be more important as it guides participants, with an unorganized knowledge base as mentioned above, to start organizing the knowledge base and see where the applications of the knowledge are at the same time. However, this is difficult again due to cost issue. Yes, current experienced staff can be the mentor but they are already swarmed with their own work. Hiring external mentor could be a solution but again, opportunity cost for the freelancer can be high if the company only require an hour from the external mentor for guidance.\u0026rdquo;\u003c/p\u003e","title":"On Mentorship"},{"content":" eth.syncing\n{ currentBlock: 13060865, highestBlock: 13060940, knownStates: 142012464, pulledStates: 141992942, startingBlock: 0 }\nAfter slightly over 38 hours, the blocks are sync-ed as of 4pm!\nI actually started over at 2am the previous day because I accidentally turned off power to the Pi and the ancient got corrupted :(\n","permalink":"https://shenting.org/post/eth_node_3/","summary":"\u003cblockquote\u003e\n\u003cp\u003eeth.syncing\u003c/p\u003e\n\u003cp\u003e{\ncurrentBlock: 13060865,\nhighestBlock: 13060940,\nknownStates: 142012464,\npulledStates: 141992942,\nstartingBlock: 0\n}\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eAfter slightly over 38 hours, the blocks are sync-ed as of 4pm!\u003c/p\u003e\n\u003cp\u003eI actually started over at 2am the previous day because I accidentally turned off power to the Pi and the ancient got corrupted :(\u003c/p\u003e","title":"ETH Node Part 3: Block Sync"},{"content":"Out of memory So I left the node running for a few days and noticed the sync rate dropping significantly. Upon watching the log print for a while, I saw the following:\nAug 13 01:51:22 geth geth[12749]: runtime: out of memory: cannot allocate 4194304-byte block (2422145024 in use)\nAug 13 01:51:22 geth geth[12749]: fatal error: out of memory\nAug 13 01:51:22 geth geth[12749]: goroutine 16172 [running]:\nAug 13 01:51:22 geth geth[12749]: runtime.throw(0xce93c8, 0xd)\nAug 13 01:51:22 geth geth[12749]: #011runtime/panic.go:1117 +0x5c fp=0xc5b5334 sp=0xc5b5320 pc=0x5c328\nAug 13 01:51:22 geth geth[12749]: runtime.(*mcache).allocLarge(0xb6c53790, 0x8026, 0x20101, 0x29e20)\nAug 13 01:51:22 geth geth[12749]: #011runtime/mcache.go:226 +0x268 fp=0xc5b5364 sp=0xc5b5334 pc=0x3a888\nAug 13 01:51:22 geth geth[12749]: runtime.mallocgc(0x8026, 0xbaee50, 0x2001, 0x2bcf04)\nAug 13 01:51:22 geth geth[12749]: #011runtime/malloc.go:1078 +0x9c8 fp=0xc5b53c0 sp=0xc5b5364 pc=0x307d8\nAug 13 01:51:22 geth geth[12749]: runtime.makeslice(0xbaee50, 0x6922, 0x8026, 0x29ddc)\nAug 13 01:51:22 geth geth[12749]: #011runtime/slice.go:98 +0x6c fp=0xc5b53d4 sp=0xc5b53c0 pc=0x75a48\nWell, that’s not good. After this repeated a few times, I googled and realised that the default Raspbian OS wouldn’t work in terms of memory addressing. The solution would be to download the image off: https://ethereum.org/en/developers/tutorials/run-node-raspberry-pi/\nWrong Image For some reason I downloaded the image for NanoPC instead of Raspberry Pi, which wrote the SD card as ext4 instead of FAT. The site took really long to download the image (averaging 200KB/s for a 700MB file took an hour zzz), but once that was done, it took 10 minutes for the initial set up.\nNow to think of it, I downloaded the wrong image because the ethraspbian side wasn’t even allowing me to download the right image the other day, and going to ethraspbian.com directed me to the wrong image instead.\nSSH The first thing I noticed was slow response to the prompt after entering the password for SSH.\nThe top result on Google suggested DNS: https://jrs-s.net/2017/07/01/slow-ssh-logins/ but this wasn’t it. The helpful reminder was to use the -vv flag to debug, which led me to this: https://serverfault.com/questions/792486/ssh-connection-takes-forever-to-initiate-stuck-at-pledge-network. Checking the ssh config file revealed that it was PAM which was causing the issue. A restart of SSH daemon and the issue disappeared.\nRunning geth as a service [Note: because I used the ETH 2.0 image, and geth isn’t installed, I had to install golang and the latest version (1.10.7 as of writing) of geth manually, following the instructions from the previous guide.]\nThis failed repeatedly, despite the command running perfectly fine. Journalctl turns up nothing, but wait, a few other services are failing too, referencing to ‘/dev/sda1’. But we have no /dev/sda1, so I commented out the line in /etc/fstab, and rebooted with bated breath…no issues on reboot! And now systemctl starts geth immediately. Who would have thought it?\nAfter a few hours of troubleshooting, I have a sync-ing node again, and now I can monitor system parameters via Grafana. Let’s see how this goes!\n","permalink":"https://shenting.org/post/eth_node_2/","summary":"\u003ch1 id=\"out-of-memory\"\u003eOut of memory\u003c/h1\u003e\n\u003cp\u003eSo I left the node running for a few days and noticed the sync rate dropping significantly. Upon watching the log print for a while, I saw the following:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAug 13 01:51:22 geth geth[12749]: runtime: out of memory: cannot allocate 4194304-byte block (2422145024 in use)\u003c/p\u003e\n\u003cp\u003eAug 13 01:51:22 geth geth[12749]: fatal error: out of memory\u003c/p\u003e\n\u003cp\u003eAug 13 01:51:22 geth geth[12749]: goroutine 16172 [running]:\u003c/p\u003e\n\u003cp\u003eAug 13 01:51:22 geth geth[12749]: runtime.throw(0xce93c8, 0xd)\u003c/p\u003e","title":"ETH Node Part 2: 64 Bit Ubuntu"},{"content":"I’ve been meaning to look into running an ETH node for a while, and Roman helpfully sent me this guide, which helpfully links to another guide on running an ETH node on a Raspberry Pi 4. There was some scepticism if a Raspberry Pi 4 is actually powerful enough, but the hardware would cost me just $300 for both the Pi and an external 500GB SSD, and it’s still usable for other projects even if this doesn’t work out.\nSo a few things to note:\nAn SSD is the bare minimum to run a full node.\nThe Pi runs really hot while syncing, so a metal case with fan is recommended. I stupidly forgot to put the thermal pads so it runs around 65-75 degrees C.\nAfter the initial boot set-up making sure that SSH is running, I could just do the rest of the steps via SSH from my desktop. (WSL is a good tool!)\nWhat do you get out of running an ETH node? The short answer is: nothing, other than increasing the security and redundancy of the ETH network.\nThe slightly longer answer is: more personal security using wallets and if developing dApps. It is also easier to query the state of the blockchain without relying on a 3rd party service. However, for full data, that would require an archive node, which would require TBs of SSD storage, more RAM and more powerful CPU than what the Raspberry Pi 4 provides.\nThe Ethereum Foundation website has a more comprehensive overview.\nRunning an ETH node is different from mining! Miners process the block transactions and nodes are the ones checking if the transactions are valid.\nHow long will it take to sync? I have no idea. The first 12 hours reached 140M state entries (the current total is at least 700M?), but this rate has now slowed down to only another 35M state entries in the past 12 hours.\nOr maybe Roman is right and it will never reach full sync.\n","permalink":"https://shenting.org/post/eth_node_1/","summary":"\u003cp\u003eI’ve been meaning to look into running an ETH node for a while, and Roman helpfully sent me \u003ca href=\"https://medium.com/@JustinMLeroux/running-ethereum-full-nodes-a-guide-for-the-barely-motivated-a8a13e7a0d31\"\u003ethis guide\u003c/a\u003e, which helpfully links to \u003ca href=\"https://kauri.io/#communities/Ethereum%20Node%20Runners/running-an-ethereum-full-node-on-a-raspberrypi-4-/\"\u003eanother guide on running an ETH node on a Raspberry Pi 4\u003c/a\u003e. There was some scepticism if a Raspberry Pi 4 is actually powerful enough, but the hardware would cost me just $300 for both the Pi and an external 500GB SSD, and it’s still usable for other projects even if this doesn’t work out.\u003c/p\u003e","title":"ETH Node Part 1: Raspberry Pi 4"},{"content":"I was having a conversation with David and the subject of outsourcing came up.\nI’ll start by stating I am not against outsourcing. There are definitely situations where it makes sense, especially for resource-constrained organizations who can’t possibly cover every single function by themselves. (On a personal level, hosting this site on SquareSpace is also a form of outsourcing.)\nOutsourcing does work for one-off projects where it doesn’t make sense for an organization to hire long-term. So, if you’re working on a one-off data project, it probably makes sense to outsource the work.\nProblem 1: Most data projects are not one-off, but recurring.\nMost data projects are complex to a certain extent, and require innate understanding of how the data was generated, how the data is processed, and what business questions need to be answered. Is this knowledge easily transferred? How is the organization going to respond to changing needs for the data in the future if the knowledge is not retained within the organization, but exits together with the third party?\nProblem 2: Quality of the work is correlated with ownership/understanding\nSome might find what I’m going to say next controversial: data work is uninteresting without context and/or ownership. It is difficult to trudge through data cleaning, pre-processing, etc. without seeing what the end goal is. That’s not even considering that decisions made on how to process the data are affected by the end goal(s). I can’t speak for my peers, but I find it hard to work on projects where there is no clearly defined question/hypothesis/problem.\nSo is it any surprise that a third-party contractor (who might/might not be underpaid) delivers unoptimized/sub-par implementations after the problem has been poorly/mis-explained?\nSo here’s what I think can be done better: stakeholders need to communicate more if they are outsourcing, and at least give some ownership of the decisions to the people actually dealing with the data. Be upfront about objectives: if there is ambiguity that needs to be resolved with a first cut of the data, say so and be clear about what is needed to resolve the ambiguity. Everyone will be clearer and happier, with less time wasted.\n","permalink":"https://shenting.org/post/outsource_data_work/","summary":"\u003cp\u003eI was having a conversation with David and the subject of outsourcing came up.\u003c/p\u003e\n\u003cp\u003eI’ll start by stating I am not against outsourcing. There are definitely situations where it makes sense, especially for resource-constrained organizations who can’t possibly cover every single function by themselves. (On a personal level, hosting this site on SquareSpace is also a form of outsourcing.)\u003c/p\u003e\n\u003cp\u003eOutsourcing does work for one-off projects where it doesn’t make sense for an organization to hire long-term.  So, if you’re working on a one-off data project, it probably makes sense to outsource the work.\u003c/p\u003e","title":"Why you probably shouldn't outsource data work"},{"content":"Yes, we’re talking about actual names\nIt’s a pity that I came across this 6 months too late, as it would have saved me an hour repeating myself thrice on why we can’t use names as a unique key to join across different data sources.\nThankfully my point eventually got across, but to any fellow developer/data scientist/engineer having to explain to stakeholders, hopefully this helps.\nAnd no, you do not want to use email addresses as a unique key either:\nPeople make typos with their email addresses more often than you think, e.g. “.com” instead of “.edu”\nPeople have multiple email addresses and are inconsistent with which one they use\nPeople change email addresses more often than you think\nIf there is already an existing unique ID of sort (access card, database generated, registration number), then that should be used instead.\n","permalink":"https://shenting.org/post/names/","summary":"\u003cp\u003e\u003ca href=\"https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/\"\u003eYes, we’re talking about actual names\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIt’s a pity that I came across this 6 months too late, as it would have saved me an hour repeating myself thrice on why we can’t use names as a unique key to join across different data sources.\u003c/p\u003e\n\u003cp\u003eThankfully my point eventually got across, but to any fellow developer/data scientist/engineer having to explain to stakeholders, hopefully this helps.\u003c/p\u003e\n\u003cp\u003eAnd no, you do not want to use email addresses as a unique key either:\u003c/p\u003e","title":"Mandatory Reading on Names"},{"content":"Since the new house is relatively settled now, we were inspired by woonie’s post to try a different route.\nWe lost by less than 5 minutes, maybe we would have beaten his team if not for the toilet break at Bukit Panjang which resulted in us missing a LRT.\n","permalink":"https://shenting.org/post/mrt_challenge/","summary":"\u003cp\u003eSince the new house is relatively settled now, we were inspired by \u003ca href=\"https://www.reddit.com/r/singapore/comments/kmsbl4/challenge_accepted_122_mrt_stations_rta/\"\u003ewoonie’s post\u003c/a\u003e to try \u003ca href=\"https://www.reddit.com/r/singapore/comments/ku4nla/for_science_122_mrt_stations_6_lrt_stations/\"\u003ea different route.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe lost by less than 5 minutes, maybe we would have beaten his team if not for the toilet break at Bukit Panjang which resulted in us missing a LRT.\u003c/p\u003e","title":"MRT Challenge"},{"content":"Point vs Interval Estimates One issue with the Elo system as previously raised is how it doesn’t give any information about the uncertainty of the estimate. The Glicko system is an attempt to encompass this information additionally.\nThink about this: If a player has played 3 games: win against a player rated 1500, lose against a player rated 1450 and win against a player rated 1200, how confident are we in the derived rating? On the other hand, if a player has played 50 games, and has won 35 games against opponents rated 2000 and below, drew 5 games against opponents rated 2000-2050 and lost the remaining 10 games versus opponents rated above 2050, we can be fairly certain that his rating is somewhere between 2000-2050.\nAnd of course we now also have to take into consideration the opponent’s rating uncertainty too.\nThe Glicko-2 Algorithm The full details can be found in this tutorial by Mark Glickman himself. I’ll run through a summary of the algorithm\nDecide on two system parameters - volatility and constraint constant. Initialize rating to 1500, rating deviation (RD) to 350 and player volatility to 0.06.\nConvert rating and RD to Glicko scale. Normalize rating to 0, and divide both by 173.7178. We will consider a rating period where a player meets a number of opponents with respective ratings and RDs.\nCompute $v$, which is an estimated variance of the player’s rating based on game outcomes only. Compute $\\delta$, which is estimated improvement in rating by comparing pre-period rating vs performance rating based only on game outcomes. Determine the new value of volatility. This step requires numerical iterative methods. Update the rating deviation to the new pre-rating period value. Update the rating and RD. Convert back to original rating scale (i.e. normalised to 1500) If a player does not play during a period, then only increase the RD according to the volatility constant.\nAdvantages Measure of volatility/uncertainty\nInactivity is “penalized” with increased uncertainty\nDoesn’t rely on constant K - avoids issues of K being set wrongly\nDisadvantages Computationally complicated\nNot all players can understand two numbers instead of one for rating.\nGlicko-2 is very popular - it is used on both chess.com and lichess.com. It is also the base for rating systems in DotA, CS:GO and other esports titles.\n","permalink":"https://shenting.org/post/rating_2_glicko/","summary":"\u003ch1 id=\"point-vs-interval-estimates\"\u003ePoint vs Interval Estimates\u003c/h1\u003e\n\u003cp\u003eOne issue with the Elo system as previously raised is how it doesn’t give any information about the uncertainty of the estimate. The Glicko system is an attempt to encompass this information additionally.\u003c/p\u003e\n\u003cp\u003eThink about this: If a player has played 3 games: win against a player rated 1500, lose against a player rated 1450 and win against a player rated 1200, how confident are we in the derived rating? On the other hand, if a player has played 50 games, and has won 35 games against opponents rated 2000 and below, drew 5 games against opponents rated 2000-2050 and lost the remaining 10 games versus opponents rated above 2050, we can be fairly certain that his rating is somewhere between 2000-2050.\u003c/p\u003e","title":"Rating Systems (2): Glicko-2"},{"content":"I was invited by the Rafflesian Parents Association to give a talk on being a data scientist (as part of a five speaker panel) earlier today. Here are the slides.\nWas not totally happy as I didn’t realise that Google Sheets speaker notes covers the actual slides when I was screen sharing over zoom. Also, this is the first time I’ve actually given a career talk, and realised it was 15 years ago when I was sitting in the audience on the other side. Time flies!\n","permalink":"https://shenting.org/post/rpa_career_talk/","summary":"\u003cp\u003eI was invited by the Rafflesian Parents Association to give a talk on being a data scientist (as part of a five speaker panel) earlier today. \u003ca href=\"https://docs.google.com/presentation/d/1yVgou8qDtAnGLuzDmCcoxjOIzAi48JJdgtd5MWqeruk/edit?usp=sharing\"\u003eHere are the slides.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWas not totally happy as I didn’t realise that Google Sheets speaker notes covers the actual slides when I was screen sharing over zoom. Also, this is the first time I’ve actually given a career talk, and realised it was 15 years ago when I was sitting in the audience on the other side. Time flies!\u003c/p\u003e","title":"RPA Career Talk"},{"content":"Introduction This is going to be a new series on rating systems, which is a vastly underrated (pun intended) area of statistics and data science. Rating systems has actually been a part of my life (and probably yours), from early days in chess and then games with matchmaking like CS:GO and Valorant, to now thinking if contract bridge should also have one.\nHistorical Context Having been around for centuries, chess is a game which people have wasted much time on arguing/debating who is the best player. It’s probably slightly surprising then that the first modern rating systems only appeared around or after the end of World War 2. The first systems (Ingo and Harkness) were quite simple and used the idea of the average rating of opponents with adjustments for the results.\nIt is worth noting that the English Chess Federation still has a rating system still in operation (since 1958) with somewhat similar ideas: a player’s grade is his opponent’s grade +- 50 (depending on win, or loss; it’s the opponent’s grade if it’s a draw). The ECF system also caps the difference to 40 points between both players (even if the actual difference is more than 40), and the player’s rating is the average over all the matches during a time period.\nECF System The ECF system, while limited, actually contains some fundamental ideas:\nEqually strong opponents will draw (might not be true for other games), or have an even chance of winning.\nWinning results in gain in rating, losing results in a loss. The outcomes are binary.\nSome form of capping is needed to prevent huge changes in rating, or even worse, higher rated players losing rating even after a win.\nAveraging over a time period.\nOne big criticism is how the ECF rating system is a “lagging” indicator, especially for junior players who improve faster than the rating system can catch up.\nElo System The rating system takes its name after it’s inventor, Arpad Elo, a Hungarian-American professor of physics who was also a chess player.\nExpected Score The main idea of the rating system is that of the expected score, which is the sum of the probability of winning plus half the probability of drawing (in chess, a score of 1 is given for a win and 0.5 for a draw). Elo suggested scaling the ratings such that a difference of 200 points would give an expected score of 0.75 for the stronger player. The average rating of 1500 was chosen by the US Chess Federation and this is used as the initial rating. Note that no distinction is made between wins and draws in this expected value; a draw is simply half a win.\nThe expected score is simply a logistic function, for example the expected score of Player A and Player B is given below, where R are the respective ratings:\n$$ \\ E_A = \\frac{1}{1 + 10^{\\frac{R_B - R_A}{400}}} $$ $$ \\ E_B = \\frac{1}{1 + 10^{\\frac{R_A - R_B}{400}}} $$\nNote that the two equations are symmetrical.\nThe rating is then updated by multiplying the difference between the actual and expected score by a K factor.\nElo originally set K=10, which is deemed to be too low, i.e. too insensitive/lagging behind actual performance. The FIDE tiers this to three different levels:\nK = 40, for new and young players aged under 18\nK = 20, for players rated below 2400\nK= 10, for players rated above 2400, and at least 30 games played in previous events.\nMathematical Issues The normal distribution is easy to understand, and symmetrical, but in practice the US Chess Federation found the logistic distribution a better fit.\nThe correct value of K, as stated above.\nThe Elo rating is a point estimator, without any indication of uncertainty. This is problematic for players who are new or players returning from a long period of inactivity.\nPractical Issues Players can choose not to play to protect their high rating, which is undesirable for competition.\nPlayers might choose their choice of opponents to minimize risk and maximize rewards\n","permalink":"https://shenting.org/post/rating_1_elo/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThis is going to be a new series on rating systems, which is a vastly underrated (pun intended) area of statistics and data science. Rating systems has actually been a part of my life (and probably yours), from early days in chess and then games with matchmaking like CS:GO and Valorant, to now thinking if contract bridge should also have one.\u003c/p\u003e\n\u003ch1 id=\"historical-context\"\u003eHistorical Context\u003c/h1\u003e\n\u003cp\u003eHaving been around for centuries, chess is a game which people have wasted much time on arguing/debating who is the best player. It’s probably slightly surprising then that the first modern rating systems only appeared around or after the end of World War 2. The first systems (Ingo and Harkness) were quite simple and used the idea of the average rating of opponents with adjustments for the results.\u003c/p\u003e","title":"Rating Systems (1): Elo and its limitations"},{"content":"A while ago, I wrote about the GE2020 sample count. Together with Yong Sheng,[] we gave a talk about this at DataScience SG last night (Youtube link)](https://www.youtube.com/watch?v=U9-zax0mMrw). Do also check out the second talk as reinforcement learning is always an interesting subject - props to Siddarth for giving that quick summary of RL!\nAlso, Symbolic Connection’s episode featuring me is now live!\nThanks to Koo Ping Shung for organizing both of the above.\nWith these two out of the way, hopefully I have the time to get back to rating systems…\n","permalink":"https://shenting.org/post/podcast_dssg/","summary":"\u003cp\u003eA while ago, I wrote about the \u003ca href=\"sample_count_2\"\u003eGE2020 sample count\u003c/a\u003e. Together with Yong Sheng,[] we gave a talk about this at DataScience SG last night (Youtube link)](\u003ca href=\"https://www.youtube.com/watch?v=U9-zax0mMrw)\"\u003ehttps://www.youtube.com/watch?v=U9-zax0mMrw)\u003c/a\u003e. Do also check out the second talk as reinforcement learning is always an interesting subject - props to Siddarth for giving that quick summary of RL!\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://anchor.fm/symbolic-connection/episodes/015--Ang-Shen-Ting--Data-Scientist-with-INSEAD-ejep1e/a-a1pvsb\"\u003eAlso, Symbolic Connection’s episode featuring me is now live!\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThanks to Koo Ping Shung for organizing both of the above.\u003c/p\u003e","title":"DataScience SG Talk on Sample Count and Podcast"},{"content":"Election season is upon us again here in Singapore and Polling Day is this Friday.\nThe last election in 2015 introduced the sample count. What is the sample count? From the ELD Website:\nFrom the votes cast at each polling station, a counting assistant picks up a random bundle of 100 ballot papers (in front of the candidates and counting agents present) and counts the number of votes for each candidate (or group of candidates in the case of a GRC).\nThe votes will be added up, with weightage given to account for the difference in the number of votes cast at each polling station.\nSample count for the electoral division will be shown as a percentage of valid votes garnered by each candidate (or group of candidates).\nThe next question would be how many votes are actually counted? This depends on the number of polling stations in a SMC/GRC as only 100 ballot papers are randomly sampled from each polling station.\nThere’s a very helpful article written back in 2015 by Ngiam Shih-Tung which actually gives more helpful details. SMCs have 5-11 polling stations, with an average of 9, while GRCs have 32-66 polling stations with an average of 45. This actually significantly affects the uncertainty of the sample count.\nIf you’ve read the article and wonder how Shih-Tung arrived at the figures, here’s the explanation:\nIf we assume that votes are binary (discounting spoilt votes), then this is simply a binomial distribution sampling. Using the standard normal distribution, for a 95% confidence interval, this is 1.96 times the standard deviation, which is:\nIndependent of n, the number of samples, the sample standard deviation is the largest when the sample probability is 0.5. The other thing of note is the standard deviation decreases as n increases. Thus, we will expect higher error margins with SMCs compared to GRCs.\nSo, taking the example of Potong Pasir, the maximum standard deviation would be obtained by using p = 0.5 and n = 500, which is 0.02236. Multiplying that by 1.96 gives 0.0438, or 4.38%.\nThis means that if the vote is dead even, the sample count has a 95% chance of falling within 45.62%-54.38%. In contrast, for Pasir Ris-Punggol GRC, this interval is a lot narrower at roughly 48.8%-51.2%.\nHence, it is important to know how many polling stations and hence the number of samples to determine the margin of error to interpret the sample count results. If you look at table 2 of the article, you can see that in most cases, the result is a foregone conclusion after the sample count.\n","permalink":"https://shenting.org/post/sample_count_1/","summary":"\u003cp\u003eElection season is upon us again here in Singapore and Polling Day is this Friday.\u003c/p\u003e\n\u003cp\u003eThe last election in 2015 introduced the sample count. What is the sample count? \u003ca href=\"https://www.eld.gov.sg/mediarelease/SampleCount_Generic.pdf\"\u003eFrom the ELD Website\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003eFrom the votes cast at each polling station, a counting assistant picks up a random bundle of 100 ballot papers (in front of the candidates and counting agents present) and counts the number of votes for each candidate (or group of candidates in the case of a GRC).\u003c/p\u003e","title":"How Much Should You Trust the Sample Count?"},{"content":"(Note: This was originally posted on Facebook)\nTL;DR While the sample counts of this year’s GE saw some huge deviations, they are mostly within expectation.\nThe two hour extension brought about some online groans from friends who didn’t want to stay up late to follow the results. To help alleviate some of this anxiety, I decided to set up a tracking sheet for the sample counts with estimated win probabilities to let people decide if the final count was worth waiting for.\nAfter the sample counts were in, there were a few constituencies left in play: Marymount, West Coast, Bukit Panjang, Sengkang and Bukit Batok. Even this is generous - the latter 3 were already \u0026gt;99.99% certain by my estimate. Probably the most uncertain event was whether the PVP team in Pasir Ris-Punggol would get to keep their deposit.\nWhat was interesting as the actual counts came in were the large deviations from the sample count seen in some constituencies - Kebun Bahru was 5% off, for example. Moreover, the large deviations were mostly against the PAP. This sparked off some discussions amongst my friends.\nAnalysis My first attempt was to set up two-sided hypothesis tests (using normal approximation) if the sample proportion equals the actual proportion (Sheet “Two-sided Normal Hypothesis Test” in the link above). At the 5% level, there are a number of significant findings (note: the three-sided battles each have their own p-value for their proportion as these are not symmetric unlike the direct contests.) Kebun Bahru and Sembawang have P-values of 0.01 or less.\nWhat if we want to test for a deviation just against the PAP vote share, i.e. the alternative hypothesis that the actual vote share is less than the sample count proportion? This would be a one-sided hypothesis test, and now Pioneer also has a P-value of less than 0.01.\nCorrecting for Multiple Testing Getting one or more statistically significant result out of 30+ does not necessarily mean there is an issue. After all, there is a 5% chance of an individual test being statistically significant just by chance. A popular method to correct for this is the False Discovery Rate (FDR) procedure, by Benjamini and Hochberg.\nUsing this procedure for both two-sided and one-sided tests yields only Kebun Bahru as being statistically significant, and only in the case of the one-sided test.\nA better model - incomplete beta Yong Sheng tried using an incomplete beta distribution, which is an exact model (essentially: “draw n samples from a Bernoulli trial with probability of success p. What is the probability that of getting \u0026gt; k samples?” - this is one-sided). Applying the FDR procedure to the p-values of the incomplete beta, again only Kebun Bahru was statistically significant.\nWhat does this all mean? Perfectly random mixing is hard in real life. From various friends who volunteered (or “volunteered”), the boxes are emptied and then mixed on the table before the lucky 100 samples are taken. To quote one of them: “We play mahjong”.\nThis year’s election is somewhat special - senior citizens were encouraged to vote in the morning and the younger electorate would stick to assigned time slots in the afternoon. In practice, this would mean that the ballots at the bottom of the box are at the top of the pile of the table after emptying. Might this hypothesis be true? Maybe, but we have no way of proving it. Neither should this take anything away from the best efforts of the officials involved in the counting, who faced an even harder task this year.\nWhile the presence of large deviations might be alarming, the analysis actually shows that these are within reasonable expectation and the sample counts are reliable.\n(Thanks to the various other friends involved in the discussions on this, and thanks to anyone reading this who were involved in Polling Day, you guys are the real unsung heroes of democracy!)\n","permalink":"https://shenting.org/post/sample_count_2/","summary":"\u003cp\u003e(Note: This was originally posted on Facebook)\u003c/p\u003e\n\u003cp\u003eTL;DR While the sample counts of this year’s GE saw some huge deviations, they are mostly within expectation.\u003c/p\u003e\n\u003cp\u003eThe two hour extension brought about some online groans from friends who didn’t want to stay up late to follow the results. To help alleviate some of this anxiety, \u003ca href=\"https://docs.google.com/spreadsheets/d/198pBnso8oFpNSB2vNOSi-Cv5ZtIZl7lPvdk0_syZd6M/\"\u003eI decided to set up a tracking sheet for the sample counts with estimated win probabilities to let people decide if the final count was worth waiting for.\u003c/a\u003e\u003c/p\u003e","title":"How Much Should You Trust the Sample Count?"},{"content":"In the past 8 months, I’ve probably worked on close to 10 different projects. While half of these consists of not more than a few Jupyter notebooks, the others consist of intermediate data and different notebooks for preprocessing and modelling.\nCookiecutter seems to be a good solution and framework: https://drivendata.github.io/cookiecutter-data-science/\nRefactoring those projects will take some effort, but I believe it will be well worth the time to do so.\n","permalink":"https://shenting.org/post/organizing_ds_projects/","summary":"\u003cp\u003eIn the past 8 months, I’ve probably worked on close to 10 different projects. While half of these consists of not more than a few Jupyter notebooks, the others consist of intermediate data and different notebooks for preprocessing and modelling.\u003c/p\u003e\n\u003cp\u003eCookiecutter seems to be a good solution and framework: \u003ca href=\"https://drivendata.github.io/cookiecutter-data-science/\"\u003ehttps://drivendata.github.io/cookiecutter-data-science/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRefactoring those projects will take some effort, but I believe it will be well worth the time to do so.\u003c/p\u003e","title":"Organizing Data Science Projects"},{"content":"About Shen Ting Shen Ting is a Data Scientist with more than a decade of experience in the field, having worked at the Institute for Infocomm Research, GovTech, CureMetrix Inc., Wecash. Currently, Shen Ting is Principal Data Scientist at INSEAD (Asia Campus) in Singapore. Shen Ting has a Masters in Computer Science from UC San Diego, and a BSc Masters in MORSE (Mathematics, Operations Research, Statistics and Economics) from the University of Warwick.\nShen Ting was part of the Warwick Alumni team which placed 4th in the National Data Science Challenge in 2019.\nShen Ting was previously Competition Secretary of the Singapore Contract Bridge Association (2019-2023), and is an APBF International Tournament Director since Apr 2024.\nLinkedIn\n","permalink":"https://shenting.org/about/","summary":"\u003ch2 id=\"about-shen-ting\"\u003eAbout Shen Ting\u003c/h2\u003e\n\u003cp\u003eShen Ting is a Data Scientist with more than a decade of experience in the field, having worked at the Institute for Infocomm Research, GovTech, CureMetrix Inc., Wecash. Currently, Shen Ting is Principal Data Scientist at INSEAD (Asia Campus) in Singapore. Shen Ting has a Masters in Computer Science from UC San Diego, and a BSc Masters in MORSE (Mathematics, Operations Research, Statistics and Economics) from the University of Warwick.\u003c/p\u003e","title":"About Shen Ting"}]
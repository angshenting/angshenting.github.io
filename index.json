[{"content":"The SCBA Ratings page is live!\nThus far, only matchpoint pairs games are rated, from 2023 onwards. Most of the development was done in a week with the help of ChatGPT and Claude.\nWhy 2023? Given that SCBA re-opened in May 2022 after COVID-19 restrictions were eased, 2023 seems like a good starting point when most people returned to weekly games.\nHow is this done? Ratings are implemented using OpenSkill, using Plackett-Luce based on rankings.\nWhat about teams? In the process of implementation, most likely using Glicko-2 (see previous post) on head to head matches with weighting for number of boards.\n","permalink":"https://shenting.org/post/scba_rating_live/","summary":"\u003cp\u003e\u003ca href=\"https://dev1.scba.org.sg/ratings\"\u003eThe SCBA Ratings page is live!\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThus far, only matchpoint pairs games are rated, from 2023 onwards. Most of the development was done in a week with the help of ChatGPT and Claude.\u003c/p\u003e\n\u003ch2 id=\"why-2023\"\u003eWhy 2023?\u003c/h2\u003e\n\u003cp\u003eGiven that SCBA re-opened in May 2022 after COVID-19 restrictions were eased, 2023 seems like a good starting point when most people returned to weekly games.\u003c/p\u003e\n\u003ch2 id=\"how-is-this-done\"\u003eHow is this done?\u003c/h2\u003e\n\u003cp\u003eRatings are implemented using \u003ca href=\"https://openskill.me/en/stable/\"\u003eOpenSkill\u003c/a\u003e, using Plackett-Luce based on rankings.\u003c/p\u003e","title":"SCBA Ratings - Now Live"},{"content":"The Death of the American Dream I recall watching the movie adaptation of The Great Gatsby back in 2013. Baz Luhrmann created a visual spectacle, featuring Leonardo DiCaprio as the titular character who has become part of multiple memes. And who can forget Lana Del Rey\u0026rsquo;s haunting vocals for the theme \u0026ldquo;Young and Beautiful\u0026rdquo;? I read the original novel soon after, and was pretty struck by how deep the underlying themes run. Fast forward to 2025, and it has been exactly a century since F. Scott Fitzgerald published The Great Gatsby. One hundred years later, its themes feel eerily familiar. The excessive wealth, the disillusionment, the relentless pursuit of a dream—all of it maps almost perfectly onto today’s tech-fueled, crypto-charged, influencer-driven landscape. This struck me on a commute home last week and as I thought about it, the main characters do reflect segments of modern day society.\nTom Buchanan = Trump/Vance/Musk etc. Tom is the gatekeeper of old power. He is loud, aggressive, and entitled—living proof that wealth doesn’t equal class. He dominates conversations and people, and he always lands on his feet. He is brutish and cruel, and infidelity comes naturally to him.\nDaisy Buchanan = Old Money/GOP Daisy is the ultimate prize—beautiful, elusive, and hollow. Gatsby\u0026rsquo;s fixation isn\u0026rsquo;t with her personality, but with the status and security she represents. In 2025, Daisy represents the old political elite, the traditional Republican establishment, the donor class, and old money. She doesn’t take action; she lets others fight for her attention and then quietly retreats to her insulated world. Myrtle Wilson = Poor Populist/MAGA supporters Myrtle wants out. She wants to escape the greyness of her world and step into one of glamor and power. But she’s used, discarded, and ultimately destroyed by the very system she tries to join. She chases after Tom, believing that he will bring her a life of glamour, but only to be run over by Daisy who is driving Gatsby\u0026rsquo;s car.\nGeorge Wilson = Other Poor Blissfully ignorant of the affair his wife (fellow poor) are having with Tom, a man from Old Money, he becomes a pawn in the rivalry between Tom and Gatsby and ends up blaming and killing Gatsby for the death of his wife, before ending his own. Who then does Gatsby represent?\nJay Gatsby = Crypto Bros/Tech Founders Gatsby was a self-made man, though through means which are not exactly deemed as completely honest. His pursuit of Daisy represents his desire for status in society; similarly, the glitzy parties he threw were to chase the image. Does he really love Daisy? Probably not as much as he is in love with the ideal of what Daisy represented. Today\u0026rsquo;s Gatsby is the tech founder or crypto bro chasing legitimacy and glory. He came from nothing, made his millions (or billions) through unconventional means, and now seeks social capital to match his financial one. He is obsessed with status symbols, but beneath the surface lies a gnawing emptiness—a desire to be seen and accepted by a world that may never truly embrace him.\nConclusion Fitzgerald\u0026rsquo;s tale is timeless - humans have been chasing glamour, status and power. However, history does seem to run in cycles - as the world today seems to be on a precipice into a Great Depression-style recession, wars raging/threatening to rage around the world - we seem to have come full circle back into the disenchantment and the end of an era. If we follow the storyline, it\u0026rsquo;s clear who we should not be - Gatsby\u0026rsquo;s tale of dream-chasing ends up in tragedy and he gets blamed for the death of Myrtle even though he is innocent, and almost nobody really mourns his death. If you agree with the above, then perhaps, like me, you\u0026rsquo;ve become Nick and maybe you might be haunted by the death of Gatsby in the future. P.S. Who Jordan Baker represents is left as an open question - I have my own ideas.\n","permalink":"https://shenting.org/post/thegreatgatsby/","summary":"\u003ch1 id=\"the-death-of-the-american-dream\"\u003eThe Death of the American Dream\u003c/h1\u003e\n\u003cp\u003eI recall watching the movie adaptation of The Great Gatsby back in 2013. Baz Luhrmann created a visual spectacle, featuring Leonardo DiCaprio as the titular character who has become part of multiple memes. And who can forget Lana Del Rey\u0026rsquo;s haunting vocals for the theme \u0026ldquo;Young and Beautiful\u0026rdquo;? I read the original novel soon after, and was pretty struck by how deep the underlying themes run.\nFast forward to 2025, and it has been exactly a century since F. Scott Fitzgerald published The Great Gatsby. One hundred years later, its themes feel eerily familiar. The excessive wealth, the disillusionment, the relentless pursuit of a dream—all of it maps almost perfectly onto today’s tech-fueled, crypto-charged, influencer-driven landscape. This struck me on a commute home last week and as I thought about it, the main characters do reflect segments of modern day society.\u003c/p\u003e","title":"The Great Gatsby: 100 Years Later"},{"content":"I gave a talk last night at Data Science SG entitled “Trustable Data: Challenges in a National Sports Association”. It gives an outline of what I’ve encountered and done in the past few years for SCBA. Talk slides can be found here\n","permalink":"https://shenting.org/post/datachallenges/","summary":"\u003cp\u003eI gave a talk last night at Data Science SG entitled “Trustable Data: Challenges in a National Sports Association”. It gives an outline of what I’ve encountered and done in the past few years for SCBA. \u003ca href=\"https://docs.google.com/presentation/d/1h88SZ2S25xFc9JAjCdBfWVK-edYJj_btPIN5gI2Za-A/edit\"\u003eTalk slides can be found here\u003c/a\u003e\u003c/p\u003e","title":"DataScience SG Talk: Data Challenges"},{"content":"I was having a conversation with David and the subject of outsourcing came up.\nI’ll start by stating I am not against outsourcing. There are definitely situations where it makes sense, especially for resource-constrained organizations who can’t possibly cover every single function by themselves. (On a personal level, hosting this site on SquareSpace is also a form of outsourcing.)\nOutsourcing does work for one-off projects where it doesn’t make sense for an organization to hire long-term. So, if you’re working on a one-off data project, it probably makes sense to outsource the work.\nProblem 1: Most data projects are not one-off, but recurring.\nMost data projects are complex to a certain extent, and require innate understanding of how the data was generated, how the data is processed, and what business questions need to be answered. Is this knowledge easily transferred? How is the organization going to respond to changing needs for the data in the future if the knowledge is not retained within the organization, but exits together with the third party?\nProblem 2: Quality of the work is correlated with ownership/understanding\nSome might find what I’m going to say next controversial: data work is uninteresting without context and/or ownership. It is difficult to trudge through data cleaning, pre-processing, etc. without seeing what the end goal is. That’s not even considering that decisions made on how to process the data are affected by the end goal(s). I can’t speak for my peers, but I find it hard to work on projects where there is no clearly defined question/hypothesis/problem.\nSo is it any surprise that a third-party contractor (who might/might not be underpaid) delivers unoptimized/sub-par implementations after the problem has been poorly/mis-explained?\nSo here’s what I think can be done better: stakeholders need to communicate more if they are outsourcing, and at least give some ownership of the decisions to the people actually dealing with the data. Be upfront about objectives: if there is ambiguity that needs to be resolved with a first cut of the data, say so and be clear about what is needed to resolve the ambiguity. Everyone will be clearer and happier, with less time wasted.\n","permalink":"https://shenting.org/post/outsource_data_work/","summary":"\u003cp\u003eI was having a conversation with David and the subject of outsourcing came up.\u003c/p\u003e\n\u003cp\u003eI’ll start by stating I am not against outsourcing. There are definitely situations where it makes sense, especially for resource-constrained organizations who can’t possibly cover every single function by themselves. (On a personal level, hosting this site on SquareSpace is also a form of outsourcing.)\u003c/p\u003e\n\u003cp\u003eOutsourcing does work for one-off projects where it doesn’t make sense for an organization to hire long-term.  So, if you’re working on a one-off data project, it probably makes sense to outsource the work.\u003c/p\u003e","title":"Why you probably shouldn't outsource data work"},{"content":"Yes, we’re talking about actual names\nIt’s a pity that I came across this 6 months too late, as it would have saved me an hour repeating myself thrice on why we can’t use names as a unique key to join across different data sources.\nThankfully my point eventually got across, but to any fellow developer/data scientist/engineer having to explain to stakeholders, hopefully this helps.\nAnd no, you do not want to use email addresses as a unique key either:\nPeople make typos with their email addresses more often than you think, e.g. “.com” instead of “.edu”\nPeople have multiple email addresses and are inconsistent with which one they use\nPeople change email addresses more often than you think\nIf there is already an existing unique ID of sort (access card, database generated, registration number), then that should be used instead.\n","permalink":"https://shenting.org/post/names/","summary":"\u003cp\u003e\u003ca href=\"https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/\"\u003eYes, we’re talking about actual names\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIt’s a pity that I came across this 6 months too late, as it would have saved me an hour repeating myself thrice on why we can’t use names as a unique key to join across different data sources.\u003c/p\u003e\n\u003cp\u003eThankfully my point eventually got across, but to any fellow developer/data scientist/engineer having to explain to stakeholders, hopefully this helps.\u003c/p\u003e\n\u003cp\u003eAnd no, you do not want to use email addresses as a unique key either:\u003c/p\u003e","title":"Mandatory Reading on Names"},{"content":"Point vs Interval Estimates One issue with the Elo system as previously raised is how it doesn’t give any information about the uncertainty of the estimate. The Glicko system is an attempt to encompass this information additionally.\nThink about this: If a player has played 3 games: win against a player rated 1500, lose against a player rated 1450 and win against a player rated 1200, how confident are we in the derived rating? On the other hand, if a player has played 50 games, and has won 35 games against opponents rated 2000 and below, drew 5 games against opponents rated 2000-2050 and lost the remaining 10 games versus opponents rated above 2050, we can be fairly certain that his rating is somewhere between 2000-2050.\nAnd of course we now also have to take into consideration the opponent’s rating uncertainty too.\nThe Glicko-2 Algorithm The full details can be found in this tutorial by Mark Glickman himself. I’ll run through a summary of the algorithm\nDecide on two system parameters - volatility and constraint constant. Initialize rating to 1500, rating deviation (RD) to 350 and player volatility to 0.06.\nConvert rating and RD to Glicko scale. Normalize rating to 0, and divide both by 173.7178. We will consider a rating period where a player meets a number of opponents with respective ratings and RDs.\nCompute v, which is an estimated variance of the player’s rating based on game outcomes only.\nCompute delta, which is estimated improvement in rating by comparing pre-period rating vs performance rating based only on game outcomes.\nDetermine the new value of volatility. This step requires numerical iterative methods.\nUpdate the rating deviation to the new pre-rating period value .\nUpdate the rating and RD.\nConvert back to original rating scale (i.e. normalised to 1500)\nIf a player does not play during a period, then only run step 6, increasing the RD according to the volatility constant.\nAdvantages Measure of volatility/uncertainty\nInactivity is “penalized” with increased uncertainty\nDoesn’t rely on constant K - avoids issues of K being set wrongly\nDisadvantages Computationally complicated\nNot all players can understand two numbers instead of one for rating.\nGlicko-2 is very popular - it is used on both chess.com and lichess.com. It is also the base for rating systems in DotA, CS:GO and other esports titles.\n","permalink":"https://shenting.org/post/rating_2_glicko/","summary":"\u003ch1 id=\"point-vs-interval-estimates\"\u003ePoint vs Interval Estimates\u003c/h1\u003e\n\u003cp\u003eOne issue with the Elo system as previously raised is how it doesn’t give any information about the uncertainty of the estimate. The Glicko system is an attempt to encompass this information additionally.\u003c/p\u003e\n\u003cp\u003eThink about this: If a player has played 3 games: win against a player rated 1500, lose against a player rated 1450 and win against a player rated 1200, how confident are we in the derived rating? On the other hand, if a player has played 50 games, and has won 35 games against opponents rated 2000 and below, drew 5 games against opponents rated 2000-2050 and lost the remaining 10 games versus opponents rated above 2050, we can be fairly certain that his rating is somewhere between 2000-2050.\u003c/p\u003e","title":"Rating Systems (2): Glicko-2"},{"content":"I was invited by the Rafflesian Parents Association to give a talk on being a data scientist (as part of a five speaker panel) earlier today. Here are the slides.\nWas not totally happy as I didn’t realise that Google Sheets speaker notes covers the actual slides when I was screen sharing over zoom. Also, this is the first time I’ve actually given a career talk, and realised it was 15 years ago when I was sitting in the audience on the other side. Time flies!\n","permalink":"https://shenting.org/post/rpa_career_talk/","summary":"\u003cp\u003eI was invited by the Rafflesian Parents Association to give a talk on being a data scientist (as part of a five speaker panel) earlier today. \u003ca href=\"https://docs.google.com/presentation/d/1yVgou8qDtAnGLuzDmCcoxjOIzAi48JJdgtd5MWqeruk/edit?usp=sharing\"\u003eHere are the slides.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWas not totally happy as I didn’t realise that Google Sheets speaker notes covers the actual slides when I was screen sharing over zoom. Also, this is the first time I’ve actually given a career talk, and realised it was 15 years ago when I was sitting in the audience on the other side. Time flies!\u003c/p\u003e","title":"RPA Career Talk"},{"content":"Introduction This is going to be a new series on rating systems, which is a vastly underrated (pun intended) area of statistics and data science. Rating systems has actually been a part of my life (and probably yours), from early days in chess and then games with matchmaking like CS:GO and Valorant, to now thinking if contract bridge should also have one.\nHistorical Context Having been around for centuries, chess is a game which people have wasted much time on arguing/debating who is the best player. It’s probably slightly surprising then that the first modern rating systems only appeared around or after the end of World War 2. The first systems (Ingo and Harkness) were quite simple and used the idea of the average rating of opponents with adjustments for the results.\nIt is worth noting that the English Chess Federation still has a rating system still in operation (since 1958) with somewhat similar ideas: a player’s grade is his opponent’s grade +- 50 (depending on win, or loss; it’s the opponent’s grade if it’s a draw). The ECF system also caps the difference to 40 points between both players (even if the actual difference is more than 40), and the player’s rating is the average over all the matches during a time period.\nECF System The ECF system, while limited, actually contains some fundamental ideas:\nEqually strong opponents will draw (might not be true for other games), or have an even chance of winning.\nWinning results in gain in rating, losing results in a loss. The outcomes are binary.\nSome form of capping is needed to prevent huge changes in rating, or even worse, higher rated players losing rating even after a win.\nAveraging over a time period.\nOne big criticism is how the ECF rating system is a “lagging” indicator, especially for junior players who improve faster than the rating system can catch up.\nElo System The rating system takes its name after it’s inventor, Arpad Elo, a Hungarian-American professor of physics who was also a chess player.\nExpected Score The main idea of the rating system is that of the expected score, which is the sum of the probability of winning plus half the probability of drawing (in chess, a score of 1 is given for a win and 0.5 for a draw). Elo suggested scaling the ratings such that a difference of 200 points would give an expected score of 0.75 for the stronger player. The average rating of 1500 was chosen by the US Chess Federation and this is used as the initial rating. Note that no distinction is made between wins and draws in this expected value; a draw is simply half a win.\nThe expected score is simply a logistic function, for example the expected score of Player A and Player B is given below, where R are the respective ratings:\nNote that the two equations are symmetrical.\nThe rating is then updated by multiplying the difference between the actual and expected score by a K factor.\nElo originally set K=10, which is deemed to be too low, i.e. too insensitive/lagging behind actual performance. The FIDE tiers this to three different levels:\nK = 40, for new and young players aged under 18\nK = 20, for players rated below 2400\nK= 10, for players rated above 2400, and at least 30 games played in previous events.\nMathematical Issues The normal distribution is easy to understand, and symmetrical, but in practice the US Chess Federation found the logistic distribution a better fit.\nThe correct value of K, as stated above.\nThe Elo rating is a point estimator, without any indication of uncertainty. This is problematic for players who are new or players returning from a long period of inactivity.\nPractical Issues Players can choose not to play to protect their high rating, which is undesirable for competition.\nPlayers might choose their choice of opponents to minimize risk and maximize rewards\n","permalink":"https://shenting.org/post/rating_1_elo/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThis is going to be a new series on rating systems, which is a vastly underrated (pun intended) area of statistics and data science. Rating systems has actually been a part of my life (and probably yours), from early days in chess and then games with matchmaking like CS:GO and Valorant, to now thinking if contract bridge should also have one.\u003c/p\u003e\n\u003ch1 id=\"historical-context\"\u003eHistorical Context\u003c/h1\u003e\n\u003cp\u003eHaving been around for centuries, chess is a game which people have wasted much time on arguing/debating who is the best player. It’s probably slightly surprising then that the first modern rating systems only appeared around or after the end of World War 2. The first systems (Ingo and Harkness) were quite simple and used the idea of the average rating of opponents with adjustments for the results.\u003c/p\u003e","title":"Rating Systems (1): Elo and its limitations"},{"content":"A while ago, I wrote about the GE2020 sample count. Together with Yong Sheng,[] we gave a talk about this at DataScience SG last night (Youtube link)](https://www.youtube.com/watch?v=U9-zax0mMrw). Do also check out the second talk as reinforcement learning is always an interesting subject - props to Siddarth for giving that quick summary of RL!\nAlso, Symbolic Connection’s episode featuring me is now live!\nThanks to Koo Ping Shung for organizing both of the above.\nWith these two out of the way, hopefully I have the time to get back to rating systems…\n","permalink":"https://shenting.org/post/podcast_dssg/","summary":"\u003cp\u003eA while ago, I wrote about the \u003ca href=\"sample_count_2\"\u003eGE2020 sample count\u003c/a\u003e. Together with Yong Sheng,[] we gave a talk about this at DataScience SG last night (Youtube link)](\u003ca href=\"https://www.youtube.com/watch?v=U9-zax0mMrw)\"\u003ehttps://www.youtube.com/watch?v=U9-zax0mMrw)\u003c/a\u003e. Do also check out the second talk as reinforcement learning is always an interesting subject - props to Siddarth for giving that quick summary of RL!\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://anchor.fm/symbolic-connection/episodes/015--Ang-Shen-Ting--Data-Scientist-with-INSEAD-ejep1e/a-a1pvsb\"\u003eAlso, Symbolic Connection’s episode featuring me is now live!\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThanks to Koo Ping Shung for organizing both of the above.\u003c/p\u003e","title":"DataScience SG Talk on Sample Count and Podcast"},{"content":"Election season is upon us again here in Singapore and Polling Day is this Friday.\nThe last election in 2015 introduced the sample count. What is the sample count? From the ELD Website:\nFrom the votes cast at each polling station, a counting assistant picks up a random bundle of 100 ballot papers (in front of the candidates and counting agents present) and counts the number of votes for each candidate (or group of candidates in the case of a GRC).\nThe votes will be added up, with weightage given to account for the difference in the number of votes cast at each polling station.\nSample count for the electoral division will be shown as a percentage of valid votes garnered by each candidate (or group of candidates).\nThe next question would be how many votes are actually counted? This depends on the number of polling stations in a SMC/GRC as only 100 ballot papers are randomly sampled from each polling station.\nThere’s a very helpful article written back in 2015 by Ngiam Shih-Tung which actually gives more helpful details. SMCs have 5-11 polling stations, with an average of 9, while GRCs have 32-66 polling stations with an average of 45. This actually significantly affects the uncertainty of the sample count.\nIf you’ve read the article and wonder how Shih-Tung arrived at the figures, here’s the explanation:\nIf we assume that votes are binary (discounting spoilt votes), then this is simply a binomial distribution sampling. Using the standard normal distribution, for a 95% confidence interval, this is 1.96 times the standard deviation, which is:\nIndependent of n, the number of samples, the sample standard deviation is the largest when the sample probability is 0.5. The other thing of note is the standard deviation decreases as n increases. Thus, we will expect higher error margins with SMCs compared to GRCs.\nSo, taking the example of Potong Pasir, the maximum standard deviation would be obtained by using p = 0.5 and n = 500, which is 0.02236. Multiplying that by 1.96 gives 0.0438, or 4.38%.\nThis means that if the vote is dead even, the sample count has a 95% chance of falling within 45.62%-54.38%. In contrast, for Pasir Ris-Punggol GRC, this interval is a lot narrower at roughly 48.8%-51.2%.\nHence, it is important to know how many polling stations and hence the number of samples to determine the margin of error to interpret the sample count results. If you look at table 2 of the article, you can see that in most cases, the result is a foregone conclusion after the sample count.\n","permalink":"https://shenting.org/post/sample_count_1/","summary":"\u003cp\u003eElection season is upon us again here in Singapore and Polling Day is this Friday.\u003c/p\u003e\n\u003cp\u003eThe last election in 2015 introduced the sample count. What is the sample count? \u003ca href=\"https://www.eld.gov.sg/mediarelease/SampleCount_Generic.pdf\"\u003eFrom the ELD Website\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003eFrom the votes cast at each polling station, a counting assistant picks up a random bundle of 100 ballot papers (in front of the candidates and counting agents present) and counts the number of votes for each candidate (or group of candidates in the case of a GRC).\u003c/p\u003e","title":"How Much Should You Trust the Sample Count?"},{"content":"(Note: This was originally posted on Facebook)\nTL;DR While the sample counts of this year’s GE saw some huge deviations, they are mostly within expectation.\nThe two hour extension brought about some online groans from friends who didn’t want to stay up late to follow the results. To help alleviate some of this anxiety, I decided to set up a tracking sheet for the sample counts with estimated win probabilities to let people decide if the final count was worth waiting for.\nAfter the sample counts were in, there were a few constituencies left in play: Marymount, West Coast, Bukit Panjang, Sengkang and Bukit Batok. Even this is generous - the latter 3 were already \u0026gt;99.99% certain by my estimate. Probably the most uncertain event was whether the PVP team in Pasir Ris-Punggol would get to keep their deposit.\nWhat was interesting as the actual counts came in were the large deviations from the sample count seen in some constituencies - Kebun Bahru was 5% off, for example. Moreover, the large deviations were mostly against the PAP. This sparked off some discussions amongst my friends.\nAnalysis My first attempt was to set up two-sided hypothesis tests (using normal approximation) if the sample proportion equals the actual proportion (Sheet “Two-sided Normal Hypothesis Test” in the link above). At the 5% level, there are a number of significant findings (note: the three-sided battles each have their own p-value for their proportion as these are not symmetric unlike the direct contests.) Kebun Bahru and Sembawang have P-values of 0.01 or less.\nWhat if we want to test for a deviation just against the PAP vote share, i.e. the alternative hypothesis that the actual vote share is less than the sample count proportion? This would be a one-sided hypothesis test, and now Pioneer also has a P-value of less than 0.01.\nCorrecting for Multiple Testing Getting one or more statistically significant result out of 30+ does not necessarily mean there is an issue. After all, there is a 5% chance of an individual test being statistically significant just by chance. A popular method to correct for this is the False Discovery Rate (FDR) procedure, by Benjamini and Hochberg.\nUsing this procedure for both two-sided and one-sided tests yields only Kebun Bahru as being statistically significant, and only in the case of the one-sided test.\nA better model - incomplete beta Yong Sheng tried using an incomplete beta distribution, which is an exact model (essentially: “draw n samples from a Bernoulli trial with probability of success p. What is the probability that of getting \u0026gt; k samples?” - this is one-sided). Applying the FDR procedure to the p-values of the incomplete beta, again only Kebun Bahru was statistically significant.\nWhat does this all mean? Perfectly random mixing is hard in real life. From various friends who volunteered (or “volunteered”), the boxes are emptied and then mixed on the table before the lucky 100 samples are taken. To quote one of them: “We play mahjong”.\nThis year’s election is somewhat special - senior citizens were encouraged to vote in the morning and the younger electorate would stick to assigned time slots in the afternoon. In practice, this would mean that the ballots at the bottom of the box are at the top of the pile of the table after emptying. Might this hypothesis be true? Maybe, but we have no way of proving it. Neither should this take anything away from the best efforts of the officials involved in the counting, who faced an even harder task this year.\nWhile the presence of large deviations might be alarming, the analysis actually shows that these are within reasonable expectation and the sample counts are reliable.\n(Thanks to the various other friends involved in the discussions on this, and thanks to anyone reading this who were involved in Polling Day, you guys are the real unsung heroes of democracy!)\n","permalink":"https://shenting.org/post/sample_count_2/","summary":"\u003cp\u003e(Note: This was originally posted on Facebook)\u003c/p\u003e\n\u003cp\u003eTL;DR While the sample counts of this year’s GE saw some huge deviations, they are mostly within expectation.\u003c/p\u003e\n\u003cp\u003eThe two hour extension brought about some online groans from friends who didn’t want to stay up late to follow the results. To help alleviate some of this anxiety, \u003ca href=\"https://docs.google.com/spreadsheets/d/198pBnso8oFpNSB2vNOSi-Cv5ZtIZl7lPvdk0_syZd6M/\"\u003eI decided to set up a tracking sheet for the sample counts with estimated win probabilities to let people decide if the final count was worth waiting for.\u003c/a\u003e\u003c/p\u003e","title":"How Much Should You Trust the Sample Count?"}]